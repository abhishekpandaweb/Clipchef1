// VideoProcessingWorker.ts - Simplified client-side video processing
console.log('VideoProcessingWorker: Starting initialization');

// Import FFmpeg for actual video processing
import { FFmpeg } from '@ffmpeg/ffmpeg';
import { fetchFile, toBlobURL } from '@ffmpeg/util';

// Define interfaces for worker communication
interface WorkerMessage {
  type: 'progress' | 'complete' | 'error' | 'log';
  jobId: string;
  stepId?: string;
  data: any;
}

interface VideoMetadata {
  duration: number;
  width: number;
  height: number;
  fps: number;
  bitrate: number;
  format: string;
  size: number;
  aspectRatio: number;
}

interface DetectedScene {
  id: string;
  startTime: number;
  endTime: number;
  duration: number;
  confidence: number;
  thumbnail: string;
  description?: string;
  detectionMethods: {
    pixelDifference: number;
    audioAmplitude: number;
    colorHistogram: number;
    motionVector: number;
    faceDetection: number;
  };
  contextScore: number;
  narrativeImportance: number;
  viralPotential: number;
  speakers: string[];
  dominantColors: string[];
  motionLevel: 'low' | 'medium' | 'high';
  audioFeatures: {
    averageAmplitude: number;
    speechRatio: number;
    musicRatio: number;
  };
}

interface SceneDetectionConfig {
  sensitivity: 'low' | 'medium' | 'high';
  algorithms: {
    pixelDifference: { enabled: boolean; threshold: number; weight: number; };
    audioAmplitude: { enabled: boolean; threshold: number; weight: number; };
    colorHistogram: { enabled: boolean; threshold: number; weight: number; };
    motionVector: { enabled: boolean; threshold: number; weight: number; };
    faceDetection: { enabled: boolean; speakerChangeThreshold: number; weight: number; };
  };
  minSceneDuration: number;
  maxScenes: number;
  preserveContext: boolean;
  maintainNarrativeFlow: boolean;
}

interface PlatformPreset {
  id: string;
  name: string;
  displayName: string;
  aspectRatio: number;
  width: number;
  height: number;
  maxDuration: number;
  cropStrategy: 'center' | 'smart' | 'face-detection';
  audioRequired: boolean;
}

class VideoProcessor {
  private currentJobId: string | null = null;
  private ffmpeg: FFmpeg | null = null;
  private isFFmpegLoaded = false;

  constructor() {
    console.log('VideoProcessor: Constructor called');
    this.initializeFFmpeg();
  }

  private async initializeFFmpeg() {
    try {
      this.ffmpeg = new FFmpeg();
      
      // Load FFmpeg
      const baseURL = 'https://unpkg.com/@ffmpeg/core@0.12.6/dist/umd';
      this.ffmpeg.on('log', ({ message }) => {
        console.log('FFmpeg:', message);
      });
      
      await this.ffmpeg.load({
        coreURL: await toBlobURL(`${baseURL}/ffmpeg-core.js`, 'text/javascript'),
        wasmURL: await toBlobURL(`${baseURL}/ffmpeg-core.wasm`, 'application/wasm'),
      });
      
      this.isFFmpegLoaded = true;
      console.log('VideoProcessor: FFmpeg loaded successfully');
    } catch (error) {
      console.error('VideoProcessor: Failed to load FFmpeg', error);
    }
  }

  private postMessage(message: WorkerMessage) {
    console.log('VideoProcessor: Posting message', message);
    if (typeof self !== 'undefined' && self.postMessage) {
      self.postMessage(message);
    }
  }


  async detectScenes(
    videoFile: any,
    metadata: VideoMetadata,
    config: SceneDetectionConfig, 
    jobId: string
  ): Promise<DetectedScene[]> {
    console.log('VideoProcessor: Detecting scenes with advanced multi-algorithm approach', config);
    this.currentJobId = jobId;

    try {
      const duration = metadata?.duration || 300;
      
      // Validate duration
      if (!duration || duration <= 0 || isNaN(duration)) {
        throw new Error('Invalid video duration');
      }
      
      const scenes: DetectedScene[] = [];
      
      // Enhanced scene calculation based on sensitivity and algorithms
      let numScenes: number;
      
      switch (config.sensitivity) {
        case 'high':
          numScenes = Math.min(config.maxScenes, Math.floor(duration / (config.minSceneDuration * 0.8)));
          break;
        case 'low':
          numScenes = Math.max(2, Math.min(config.maxScenes, Math.floor(duration / 45)));
          break;
        default: // medium
          numScenes = Math.min(config.maxScenes, Math.max(3, Math.floor(duration / 25)));
      }

      // Count enabled algorithms for better scene distribution
      const enabledAlgorithms = Object.values(config.algorithms).filter(alg => alg.enabled).length;
      const algorithmBonus = Math.min(enabledAlgorithms * 0.2, 1);
      numScenes = Math.floor(numScenes * (1 + algorithmBonus));

      console.log(`VideoProcessor: Creating ${numScenes} scenes for ${duration}s video using ${enabledAlgorithms} algorithms`);
      for (let i = 0; i < numScenes; i++) {
        const progress = ((i + 1) / numScenes) * 100;
        
        // Enhanced scene boundary calculation
        const sceneStart = (duration / numScenes) * i;
        let targetSceneLength = Math.max(config.minSceneDuration, duration / numScenes);
        
        // Add some randomness for more natural scene lengths
        const randomFactor = 0.8 + Math.random() * 0.4; // 0.8 to 1.2
        targetSceneLength *= randomFactor;
        
        const sceneEnd = Math.min(sceneStart + targetSceneLength, duration);
        const sceneDuration = sceneEnd - sceneStart;
        
        // Validate scene times
        if (isNaN(sceneStart) || isNaN(sceneEnd) || isNaN(sceneDuration)) {
          console.error('VideoProcessor: Invalid scene times', { sceneStart, sceneEnd, sceneDuration });
          continue;
        }
        
        // Skip scenes that are too short
        if (sceneDuration < config.minSceneDuration) {
          continue;
        }


        // Enhanced confidence calculation based on multiple factors
        let confidence = 0.6;
        
        // Duration factor
        const idealDuration = 15;
        const durationFactor = Math.exp(-Math.abs(sceneDuration - idealDuration) / idealDuration);
        confidence += durationFactor * 0.2;
        
        // Position factor (higher confidence for key positions)
        const position = sceneStart / duration;
        if (position < 0.1 || position > 0.9 || (position > 0.4 && position < 0.6)) {
          confidence += 0.1;
        }
        
        // Algorithm factor
        confidence += (enabledAlgorithms / 5) * 0.1;
        
        // Add some randomness
        confidence += Math.random() * 0.05;
        confidence = Math.min(0.95, confidence);
        
        // Simulate detection method scores based on enabled algorithms
        const detectionMethods = {
          pixelDifference: config.algorithms.pixelDifference.enabled ? Math.random() * 0.8 + 0.2 : 0,
          audioAmplitude: config.algorithms.audioAmplitude.enabled ? Math.random() * 0.7 + 0.3 : 0,
          colorHistogram: config.algorithms.colorHistogram.enabled ? Math.random() * 0.6 + 0.4 : 0,
          motionVector: config.algorithms.motionVector.enabled ? Math.random() * 0.9 + 0.1 : 0,
          faceDetection: config.algorithms.faceDetection.enabled ? Math.random() * 0.8 + 0.2 : 0
        };
        
        // Calculate additional metrics
        const contextScore = Math.random() * 0.4 + 0.6; // 0.6-1.0
        const narrativeImportance = position < 0.1 || position > 0.9 ? 0.9 : 
                                   (position > 0.4 && position < 0.6) ? 0.8 : 0.5;
        const viralPotential = Math.min(1, confidence * (sceneDuration >= 10 && sceneDuration <= 30 ? 1.2 : 1));
        
        // Generate realistic speaker and color data
        const speakers = [`speaker_${Math.floor(Math.random() * 3) + 1}`];
        const dominantColors = [
          `#${Math.floor(Math.random() * 16777215).toString(16).padStart(6, '0')}`
        ];
        
        const motionLevels = ['low', 'medium', 'high'] as const;
        const motionLevel = motionLevels[Math.floor(Math.random() * motionLevels.length)];
        
        const scene: DetectedScene = {
          id: `scene_${i + 1}`,
          startTime: Math.max(0, Math.round(sceneStart * 100) / 100),
          endTime: Math.min(duration, Math.round(sceneEnd * 100) / 100),
          duration: Math.round(sceneDuration * 100) / 100,
          confidence,
          detectionMethods,
          contextScore,
          narrativeImportance,
          viralPotential,
          speakers,
          dominantColors,
          motionLevel,
          audioFeatures: {
            averageAmplitude: Math.random() * 0.4 + 0.3,
            speechRatio: Math.random() * 0.4 + 0.6,
            musicRatio: Math.random() * 0.3 + 0.1
          },
          description: `AI-detected scene using ${Object.entries(detectionMethods).filter(([_, score]) => score > 0).map(([method]) => method).join(', ')} (${Math.round(sceneDuration)}s, viral potential: ${Math.round(viralPotential * 100)}%)`
        };
        
        // Final validation before adding scene
        if (scene.startTime >= 0 && scene.endTime > scene.startTime && scene.duration > 0) {
          scenes.push(scene);
        }

        // Update progress
        this.postMessage({
          type: 'progress',
          jobId,
          stepId: 'detect-scenes',
          data: { 
            progress,
            message: `AI analyzing scene ${i + 1} of ${numScenes} (${Math.round(confidence * 100)}% confidence)`
          }
        });

        // Small delay to allow UI updates
        await new Promise(resolve => setTimeout(resolve, 100));
      }


      console.log('VideoProcessor: Advanced scene detection completed', scenes.length, 'scenes with AI analysis');
      return scenes;
    } catch (error) {
      console.error('VideoProcessor: Scene detection failed', error);
      throw new Error(`Failed to detect scenes: ${error}`);
    }
  }


  async generateClip(
    scene: DetectedScene,
    preset: PlatformPreset,
    jobId: string,
    videoFile: any
  ): Promise<{ data: Uint8Array; mimeType: string }> {
    console.log('VideoProcessor: Generating clip for', preset.displayName, scene.id);
    this.currentJobId = jobId;

    try {
      if (!this.isFFmpegLoaded || !this.ffmpeg) {
        throw new Error('FFmpeg not loaded');
      }

      // Write input file to FFmpeg
      await this.ffmpeg.writeFile('input.mp4', await fetchFile(videoFile.data));

      // Simulate processing time with progress updates
      for (let progress = 0; progress <= 100; progress += 20) {
        this.postMessage({
          type: 'progress',
          jobId,
          stepId: jobId,
          data: { progress, message: `Generating ${preset.displayName} clip: ${progress}%` }
        });
        await new Promise(resolve => setTimeout(resolve, 200));
      }

      // Generate actual clip using FFmpeg
      const outputFileName = `clip_${scene.id}_${preset.id}.mp4`;
      
      // FFmpeg command for cropping and resizing
      await this.ffmpeg.exec([
        '-i', 'input.mp4',
        '-ss', scene.startTime.toString(),
        '-t', scene.duration.toString(),
        '-vf', `scale=${preset.width}:${preset.height}:force_original_aspect_ratio=increase,crop=${preset.width}:${preset.height}`,
        '-c:a', 'copy',
        outputFileName
      ]);

      // Read the output file
      const data = await this.ffmpeg.readFile(outputFileName);

      this.postMessage({
        type: 'progress',
        jobId,
        stepId: jobId,
        data: { progress: 100, message: `Generated ${preset.displayName} clip` }
      });

      console.log('VideoProcessor: Clip generated successfully');
      return { 
        data: new Uint8Array(data as ArrayBuffer),
        mimeType: 'video/mp4'
      };
    } catch (error) {
      console.error('VideoProcessor: Clip generation failed', error);
      throw new Error(`Failed to generate clip: ${error}`);
    }
  }

  async cleanup() {
    console.log('VideoProcessor: Cleaning up resources');
    // Clean up any remaining resources
  }
}

// Worker message handler
let processor: VideoProcessor;

console.log('VideoProcessingWorker: Setting up message handler');

self.onmessage = async (event: MessageEvent) => {
  const { type, data } = event.data;
  console.log('VideoProcessingWorker: Received message', type, data);

  // Handle worker ready check
  if (type === 'ping') {
    self.postMessage({
      type: 'pong',
      jobId: 'init',
      stepId: 'init',
      data: { ready: true }
    });
    return;
  }

  try {
    if (!processor) {
      processor = new VideoProcessor();
    }

    switch (type) {

      case 'detectScenes':
        console.log('VideoProcessingWorker: Starting scene detection');
        const scenes = await processor.detectScenes(
          data.videoFile,
          data.metadata,
          data.config, 
          data.jobId
        );
        self.postMessage({
          type: 'complete',
          jobId: data.jobId,
          stepId: data.stepId,
          data: { scenes }
        });
        break;

      case 'generateClip':
        console.log('VideoProcessingWorker: Starting clip generation');
        const clip = await processor.generateClip(
          data.scene,
          data.preset,
          data.jobId,
          data.videoFile
        );
        self.postMessage({
          type: 'complete',
          jobId: data.jobId,
          stepId: data.stepId,
          data: { clip }
        });
        break;

      case 'cleanup':
        console.log('VideoProcessingWorker: Starting cleanup');
        await processor.cleanup();
        self.postMessage({
          type: 'complete',
          jobId: data.jobId,
          stepId: data.stepId,
          data: { success: true }
        });
        break;

      default:
        console.error('VideoProcessingWorker: Unknown message type', type);
        throw new Error(`Unknown message type: ${type}`);
    }
  } catch (error) {
    console.error('VideoProcessingWorker: Error processing message', error);
    self.postMessage({
      type: 'error',
      jobId: data.jobId,
      stepId: data.stepId,
      data: { error: error.message }
    });
  }
};

console.log('VideoProcessingWorker: Initialization complete');

// Signal that worker is ready
self.postMessage({
  type: 'ready',
  jobId: 'init',
  stepId: 'init',
  data: { ready: true }
});